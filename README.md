# mnist-dataset

Σε αυτό το στάδιο της εργασίας έχουμε να αντιμετωπίσουμε το πρόβλημα της ταξινόμησης (classification) σε βάση δεδομένων που θα επιλέξουμε οι ίδιοι. Ο αλγόριθμος τρέχει πάνω στη βάση δεδομένων mnist, η οποία περιλαμβάνει ένα σύνολο εκπαίδευσης (train set) 60000 ψηφίων από το 0 μέχρι το 9, γραμμένων με το χέρι και ένα σύνολο τεστ (test set) 10000 ψηφίων για την πιστοποίηση της απόδοσης του αλγορίθμου. Κάθε ψηφίο (datapoint) είναι μια εικόνα 28x28 (διάστασης 28x28=784), την οποία μετατρέπουμε σε ένα διάνυσμα 784x1 για να την εισάγουμε στον αλγόριθμο.
Η ταξινόμηση στο πρώτο στάδιο πραγματοποιήθηκε με 3 διαφορετικούς τρόπους:
a)1-nearest neighbours (ή Nearest Neighbours)
b)3-nearest neighbours
c)nearest centroid.
Οι παραπάνω κατηγοριοποιητές βρίσκονται έτοιμοι στη βιβλιοθήκη Scikit-Learn, την οποία χρησιμοποιήσαμε.
Στο τέλος γίνεται η σύγκριση των 3 κατηγοριοποιητών ως προς την απόδοση (accuracy) τους, για να διαπιστωθεί ποιος από τους τρεις πέτυχε καλύτερα το διαχωρισμό των 10 κλάσεων σε σχέση με τις εισόδους x_test (10 διαφορετικά ψηφία από το 0 μέχρι το 9).
Σε πρώτη φάση φορτώνονται βιβλιοθήκες που θα χρειαστούν. Παρκάτω θα γίνει αναλυτική αναφορά όλων των βιβλιοθηκών και εργαλείων που φορτώθηκαν. Σε αυτό το σημείο θα αναλυθούν μόνο οι βιβλιοθήκες και τα εργαλεία που χρησιμοποιήθηκαν για την κατηγοριοποίηση της ενδιάμεσης εργασίας. Φορτώνεται η βιβλιοθήκη keras, η οποία περιλαμβάνει το dataset και χωρίζονται τα train και test sets. Φορτώνεται η βιβλιοθήκη numpy για να τρέξει την εντολή np.random.seed(42), ώστε να μπορεί να τρέξει ο αλγόριθμος και να έχει τα ίδια αποτελέσματα κάθε φορά, η matplotlib για την οπτικοποίηση των δεδομένων και τέλος οι κατηγοριοποιητές, οι οποίοι βρίσκονται στη βιβλιοθήκη Scikit-Learn . Έπειτα, φορτώνεται το dataset, το οποίο περιέχεται στη βιβλιοθήκη keras. Ελέγχεται η διάσταση των inputs και labels. Οπτικοποιούνται οι είσοδοι των δεδομένων εκπαίδευσης και εν συνεχεία ελέγχεται η μορφή των labels της εκπαίδευσης. Στο επόμενο στάδιο , μετατρέπεται το dataset από data frame σε διάνυσμα, κανονικοποιούνται τα δεδομένα του dataset διαιρώντας με 255 (όσες είναι οι αποχρώσεις), έτσι ώστε οι είσοδοι να έχουν τιμές απο 0 (άσπρο) μέχρι 1 (μαύρο), ενώ από 0 μέχρι 1 οι ενδιάμεσες αποχρώσεις (γκρι κλπ) και κωδικοποιούνται τα labels με την κωδικοποίηση OneHot. Έπειτα, έχοντας κάνει τη παραπάνω προεργασία, τρέχει ο αλγόριθμος 3 φορές (κάθε φορά με διαφορετικό κατηγοριοποιητή) πάνω από το test set και εκτυπώνεται κάθε φορά η απόδοση του κάθε αλγορίθμου. Έτσι, γίνεται αντιληπτό κατά πόσο κάθε αλγόριθμος είναι σε θέση να προβλέψει την κλάση (δηλαδή το ψηφίο) ενός datapoint, το οποίο εισήχθη από το test set.
Τέλος, αναλύονται τα αποτελέσματα της απόδοσης του κάθε κατηγοριοποιητή. Το μεγαλύτερο accuracy, όπως είναι φανερό, πραγματοποιήθηκε με τον κατηγοριοποιητή 3-nearest neighbors (96.95%), ενώ το μικρότερο με τον κατηγοριοποιτή nearest centroid (82.03%). Αξίζει να σημειωθεί το γεγονός ότι οι k neighbours αλγόριθμοι υλοποιήθηκαν πολύ πιο αργά σε αντίθεση με τον nearest centroid. Αυτό συμβαίνει λόγω της φύσης των αλγορίθμων, διότι στον αλγορίθμο k neighbours υπολογίζεται η απόσταση (στην συγκεκριμένη περίπτωση η ευκλείδια απόσταση, διότι δεν έχουμε τροποποιήσει τις υπερπαραμέτρους) του εισαγόμενου datapoint  σε σχέση με όλα τα υπόλοιπα του dataset, ενώ στο nearest centroid το datapoint κατηγοριοποιείται στην πιο κοντινή κλάση με βάση το πλησιέστερο κέντρο της κάθε κλάσης.  Τέλος, ανάμεσα στους 1 και 3 nearest neighbours υπάρχει η μικρή διαφορα των 0,04%.

Τελική εργασία 
Σε αυτό το στάδιο θα στηθεί πλήρως συνδεδεμένο Νευρωνικό Δίκτυο, θα εκπαιδευτεί και παράλληλα θα ελεγχτεί κατά ποσό είναι σε θέση να κατηγοριοποιεί σωστά τα ψηφία στις κλάσεις τους σε σύγκριση με άλλα Νευρωνικά Δίκτυα στα οποία έχουν αλλαχτεί κάποιες από τις παραμέτρους και σε σχέση με την κατηγοριοποίηση των προηγούμενων κατηγοριοποιητών.
Στο πρώτο κομμάτι του κώδικα έχουν φορτωθεί τα μοντέλα του Νευρωνικού Δικτύου, τα επίπεδα, η ενεργοποίηση των νευρώνων καθώς και ο optimizer SGD (Stohastic Gradient Discent), ο οποίος τροποποιεί τα βάρη κατά την εκπαίδευση του Νευρωνικού.
Στο πρώτο μοντέλο έχει οριστεί ένα κρυφό επίπεδο με 64 νευρώνες και συνάρτηση ενεργοποίησης τη λογιστική, επίπεδο εξόδου με 10 νευρώνες (αφού 10 είναι οι κλάσεις στις οποίες θα ταξινομηθούν τα πρότυπα εισόδου) και συνάρτηση ενεργοποίησης τη softmax, η οποία είναι μια γενίκευση της λογιστικής και χρησιμοποιείται, συνήθως, στο στρώμα εξόδου για να κανονικοποιήσει τις εξόδους σε κατανομή πιθανότητας. Έχει οριστεί βήμα εκπαίδευσης 0,001 και συνάρτηση κόστους το Μέσο Τετραγωνικό Σφάλμα (MSE), δηλαδή την τετραγωνική απόσταση μεταξύ των διανυσμάτων εξόδου και στόχου. 
Έχοντας ορίσει τις παραμέτρους, το δίκτυο εκπαιδεύεται για 50 εποχές  και έπειτα για άλλες 50 εποχές. Μια πρώτη παρατήρηση είναι ότι σε 100 εποχές έχει σημειωθεί πολύ χαμηλό accuracy. Αξιολογείται το μοντέλο και προκύπτουν Train accuracy 44.87% και Test accuracy 46.97%, τα οποία είναι αρκετά μικρά ποσοστά. Για να αξιολογηθεί περαιτέρω το μοντέλο τυπώνουμε την πιθανότητα των στοιχείων να έχουν προβλεφτεί σωστά και παρατηρούμε ότι το κατά μήκος του διανύσματος εξόδου και ενώ έχουμε σαν είσοδο την εικόνα με το ψηφίο 7,  οι πιθανότητες είναι σχεδόν ισόποσα κατανεμημένες, γεγονός που σημαίνει ότι ο αλγόριθμος δεν είναι να σε θέση να κατηγοριοποιήσει 7. Ιδανικά στην όγδοη θέση του διανύσματος εξόδου όπου αντιστοιχεί το ψηφίο 7 (λόγω της one hot κωδικοποίησης) θα έπρεπε να εντοπίζεται μέγιστη πιθανότητα εμφάνισης για το 7, πράγμα το οποίο δε συμβαίνει. Το ίδιο, φυσκά, συμβαίνει και με τα υπόλοιπα ψηφία.  Επίσης, εισάγοντας συγκεκριμένα το ψηφίο 0, η πιθανότητα είναι 14,63%, αρκετά μικρό ποσοστό κατηγοριοποίησης.
Με σκοπό τη βελτίωση του μοντέλου θα επιλεχτεί για συνάρτηση κόστους η  Categorical Cross Entropy και θα επαναληφθεί η εκπαίδευση για 50 + 50 εποχές με τις ίδιες παραμέτρους. Οι χρόνοι εκπαίδευσης συγκριτικά με το προηγούμενο μοντέλο είναι σχεδόν ίδιοι. Αυτό που αλλάζει δραματικά είναι το accuracy. Ήδη από τις πρώτες 10 εποχές παρατηρείται ραγδαία αύξηση του accuracy  στο 70%, η οποία φτάνει μέχρι 87% στο τέλος των 50 εποχών. Τρέχοντας τον αλγόριθμο για άλλες 50 εποχές δεν παρατηρούμε την τάση αύξησης του accuracy που είχαμε στις προηγούμενες 50 εποχές, αλλά παρατηρούμε μεγάλη τιμή του σφάλματος οπότε το δίκτυο δεν εκπαιδεύεται σωστά. Αυτό αποδικνύεται και στην αξιολόγηση όπου παρατηρείται train accuracy 89,26% και Train Loss 40,71% και test accuracy 89,85% και loss 39,10%. Αξίζει να σημειωθεί ότι το ψηφίο 5 κατηγοριοποιείται σωστά με πιθανότητα 53,2%. Εκπαιδεύοντας για άλλες 50 εποχές παρατηρούμε ότι το train loss πέφτει περίπου 5%. Μία λύση για να βελτιωθεί το πρόβλημα θα ήταν ο αλγόριθμος να τρέξει για άλλες τουλάχιστον 7 με 8 εποχές ακόμη, μία κίνηση όμως, η οποία έχει υπολογιστικό κόστος. Μειώνοντας το batch size στο μισό (δηλαδή 32), δηλαδή τις εικόνες που εισάγονται στο δίκτυο κάθε φορά, δεν παρατηρείται μεγάλη βελτίωση και το train loss μειώνεται άλλα 5%, όμως χρειάζεται τριπλάσιο χρόνο για να εκπαιδευτεί σε 50 εποχές. Στην αξιολόγηση του μοντέλου προκύπτει ότι το 5 κατηγοριοποιείται σωστά με πιθανότητα 66%(μεγαλύτερο από πριν). Τέλος μειώνονται οι νευρώνες στο κρυφό επίπεδο από 64 σε 32 και επαναλαμβάνεται η εκπαίδευση και αξιολόγηση για άλλες 50 + 50 εποχές με batch size 32.  Ο χρόνος υλοποίησης συγκριτικά με την πρώτη εκπαίδευση είναι αναμενόμενα διπλάσιος λόγω του υποδπλασσιασμού του batch size, αν και οι νευρώνες μειώθηκαν. Αξιολογώντας το μοντέλο δεν παρατηρείται μεγάλη διαφορά στα accuracy και loss, ούτε και στην κατηγοριοποίηση του 5 (59,7%)
Τέλος, επιλέγεται optimizer αντί για sgd ο adam, ο οποίος δεν εκπαιδεύει τα βάρη αλλά το learning rate. Το Νερωνικό εκπαιδεύεται για 50 εποχές με batch size 32 και 64 νευρώνες στο κρυφό επίπεδο. Τα αποτελέσματα είναι θεαματικά, καθώς παρατηρηρείται κατακόρυφη πτώση του σφάλματος ήδη από τις πρώτες 5 εποχές και αύξηση του accuracy. Ο χρόνος υλοποίησης είναι ικανοποιητικός. Αξιολογώντας το μοντέλο παρατηρείται train accuracy, loss 100%, 0,01% αντίστοιχα και test accuracy, loss 97,35%, 12,84% αντίστοιχα. Επίσης παρατηρούμε ότι τα ψηφία κατηγοριποιούνται με μεγάλη ακρίβεια, καθώς το 5 καηγοριοποιείται σωστά με πιθανότητα 99,88%. Προσθέτωντας ακόμα ένα κρυφό επίπεδο 64 νευρώνων και αυξάνοντας το batch size σε 64 το δίκτυο εκπαιδεύεται για 50 εποχές και αξιολογείται. Ο χρόνος εκπαίδευσης είναι ελαφρώς καλύτερος, τα accuracy και loss παραμμένουν στα ίδια επίπεδα με πριν και το 5 κατηγοριοποιείται σωστά με πιθανότητα 99,99%.

Συγκρίνοντας τους KNN με τα Νευρωνικά Δίκτυα της τελικής εργασίας παρατηρούμε ότι η απόδοση είναι φανερά υψηλότερη στη δεύτερη περίπτωση (μετά την τροποποίηση με τον adam).  Επίσης, όσον αφορά τον χρόνο υλοποίησης, υπερτερεί ο Nearest Centroid, δυστυχώς όμως, με πολύ μικρότερο accuracy από τον Adam. Για τους nearest 1 και 3 η εκπαίδευση (δεν είναι ακριβώς εκπαίδευση διότι το μοντέλο δε μαθαίνει, αντιθέτως τοποθετεί τα δεδομένα) πραγματοποιήθηκε κατευθείαν, ενώ η αξιολόγηση πραγματοποιήθηκε σε 12 -15 λεπτά, σε αντίθεση με τα μοντέλα της τελικής εργασίας. Ο λόγος για τον οποίο η αξιολόγηση χρειάστηκε αυτό το χρόνο αναλύθηκε στο report της ενδιάμεσης εργασίας. Συμπερασματικά, ο nearest 3 και το Νευρωνικό Δίκτυο με τον Adam είναι τα μοντέλα, τα οποία τα πήγαν καλύτερα όσον αφορά την κατηγοριοποίηση. Η διαφορετική φύση των KNN τα καθιστά πιο εύκολα και κατανοητά να προγραμματιστούν, σε αντίθεση με τα Νευρωνικά Δίκτυα, τα οποία είναι πιο πολύπλοκα και διαθέτουν πολλές παραμέτρους προς τροποποίηση. 
